{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from rl.chapter3.simple_inventory_mdp_cap import InventoryState, SimpleInventoryMDPCap\n",
    "from rl.distribution import Choose, Categorical, Constant, Distribution\n",
    "from rl.dynamic_programming import policy_iteration_result\n",
    "from rl.markov_process import NonTerminal\n",
    "from rl.markov_decision_process import FiniteMarkovDecisionProcess, MarkovDecisionProcess\n",
    "from rl.policy import RandomPolicy, DeterministicPolicy, UniformPolicy\n",
    "from typing import Dict, Tuple, TypeVar\n",
    "\n",
    "\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "S = TypeVar('S')\n",
    "A = TypeVar('A')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DynamicLearningRate:\n",
    "    alpha: float\n",
    "    beta: float\n",
    "    H: int\n",
    "\n",
    "    def evaluate(self, n: int):\n",
    "        return self.alpha / (1 + ((n - 1) / self.H) ** (self.beta))\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TabularQLearning:\n",
    "    gamma: float\n",
    "    mdp: MarkovDecisionProcess[S, A]\n",
    "    init_state_distribution: Distribution[S]\n",
    "    dlr: DynamicLearningRate\n",
    "\n",
    "    def Q_learning(self, n_steps: int = 10_000):\n",
    "        mu_policy = UniformPolicy(lambda s: self.mdp.actions(NonTerminal(s)))\n",
    "\n",
    "        Q_value_func = {\n",
    "            (state, action): 0.0\n",
    "            for state in self.mdp.non_terminal_states\n",
    "            for action in self.mdp.actions(state)\n",
    "        }\n",
    "\n",
    "        state_action_counter = {\n",
    "            (state, action): 0\n",
    "            for state in si_mdp.non_terminal_states\n",
    "            for action in si_mdp.actions(state)\n",
    "        }\n",
    "\n",
    "        state: NonTerminal[S] = self.init_state_distribution.sample()\n",
    "        step: int = 0\n",
    "        while isinstance(state, NonTerminal) and step < n_steps:\n",
    "            yield Q_value_func\n",
    "            action = mu_policy.act(state).sample()\n",
    "\n",
    "            state_action_counter[(state, action)] += 1\n",
    "            next_state, reward = self.mdp.step(state, action).sample()\n",
    "            max_q_return = max(\n",
    "                [Q_value_func[(next_state, a)] for a in self.mdp.actions(next_state)]\n",
    "            )\n",
    "            Q_value_func[(state, action)] += self.dlr.evaluate(state_action_counter[(state, action)]) * (\n",
    "                reward + self.gamma * max_q_return - Q_value_func[(state, action)]\n",
    "            )\n",
    "            step += 1\n",
    "            state = next_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(NonTerminal(state=InventoryState(on_hand=0, on_order=0)),\n",
       "  0): -41.25969540088942,\n",
       " (NonTerminal(state=InventoryState(on_hand=0, on_order=0)),\n",
       "  1): -34.719251536030235,\n",
       " (NonTerminal(state=InventoryState(on_hand=0, on_order=0)),\n",
       "  2): -34.97607893474551,\n",
       " (NonTerminal(state=InventoryState(on_hand=0, on_order=1)),\n",
       "  0): -31.17706975885415,\n",
       " (NonTerminal(state=InventoryState(on_hand=0, on_order=1)),\n",
       "  1): -27.42196901248525,\n",
       " (NonTerminal(state=InventoryState(on_hand=0, on_order=2)),\n",
       "  0): -27.887500023927497,\n",
       " (NonTerminal(state=InventoryState(on_hand=1, on_order=0)),\n",
       "  0): -32.88645535319848,\n",
       " (NonTerminal(state=InventoryState(on_hand=1, on_order=0)),\n",
       "  1): -28.590209779474293,\n",
       " (NonTerminal(state=InventoryState(on_hand=1, on_order=1)),\n",
       "  0): -28.992961832671966,\n",
       " (NonTerminal(state=InventoryState(on_hand=2, on_order=0)),\n",
       "  0): -29.841091248318016}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_capacity = 2\n",
    "user_poisson_lambda = 1.0\n",
    "user_holding_cost = 1.0\n",
    "user_stockout_cost = 10.0\n",
    "\n",
    "si_mdp: FiniteMarkovDecisionProcess[InventoryState, int] = SimpleInventoryMDPCap(\n",
    "    capacity=user_capacity,\n",
    "    poisson_lambda=user_poisson_lambda,\n",
    "    holding_cost=user_holding_cost,\n",
    "    stockout_cost=user_stockout_cost,\n",
    ")\n",
    "\n",
    "\n",
    "init_dist = Choose(si_mdp.non_terminal_states)\n",
    "dlr = DynamicLearningRate(alpha = 0.03, beta = 0.5, H = 10_000)\n",
    "\n",
    "tql = TabularQLearning(gamma=0.9, mdp = si_mdp, init_state_distribution=init_dist, dlr=dlr)\n",
    "\n",
    "for q_func in tql.Q_learning(n_steps = 100_000):\n",
    "    pass\n",
    "\n",
    "q_func"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ed53eb0584307254b0c902844bcf8da812caec015d2a7bef039740197c0a4b91"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
