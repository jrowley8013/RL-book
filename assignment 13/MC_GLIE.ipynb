{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from rl.chapter3.simple_inventory_mdp_cap import InventoryState, SimpleInventoryMDPCap\n",
    "from rl.distribution import Choose, Categorical, Constant\n",
    "from rl.dynamic_programming import policy_iteration_result\n",
    "from rl.markov_process import NonTerminal\n",
    "from rl.markov_decision_process import FiniteMarkovDecisionProcess, MarkovDecisionProcess\n",
    "from rl.policy import RandomPolicy, DeterministicPolicy, UniformPolicy\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DynamicLearningRate:\n",
    "    alpha: float\n",
    "    beta: float\n",
    "    H: int\n",
    "\n",
    "    def evaluate(self, n: int):\n",
    "        return self.alpha / (1 + ((n - 1) / self.H) ** (self.beta))\n",
    "\n",
    "class TabularMonteCarloGLIE:\n",
    "    mdp: MarkovDecisionProcess\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_capacity = 2\n",
    "user_poisson_lambda = 1.0\n",
    "user_holding_cost = 1.0\n",
    "user_stockout_cost = 10.0\n",
    "\n",
    "si_mdp: FiniteMarkovDecisionProcess[InventoryState, int] = SimpleInventoryMDPCap(\n",
    "    capacity=user_capacity,\n",
    "    poisson_lambda=user_poisson_lambda,\n",
    "    holding_cost=user_holding_cost,\n",
    "    stockout_cost=user_stockout_cost,\n",
    ")\n",
    "\n",
    "\n",
    "init_dist = Choose(si_mdp.non_terminal_states)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assemble_eps_policy(\n",
    "    k: int,\n",
    "    Q_value_func: Dict[Tuple[NonTerminal[InventoryState], int], float],\n",
    "    mdp: MarkovDecisionProcess,\n",
    "):\n",
    "    unif_policy = UniformPolicy(lambda s: si_mdp.actions(NonTerminal(s)))\n",
    "    argmax_policy_mapping = {}\n",
    "    eps = 1 / float(k)\n",
    "    for state in mdp.non_terminal_states:\n",
    "\n",
    "        act_vector = np.array([action for action in mdp.actions(state)])\n",
    "        A = len(act_vector)\n",
    "\n",
    "        rets = np.array([Q_value_func[(state, action)] for action in act_vector])\n",
    "        opt_act = act_vector[np.argmax(rets)]\n",
    "\n",
    "        argmax_policy_mapping[state] = opt_act\n",
    "    argmax_policy = DeterministicPolicy(lambda s: argmax_policy_mapping[NonTerminal(s)])\n",
    "\n",
    "    policy_mapping = Categorical({argmax_policy: 1 - eps, unif_policy: eps})\n",
    "    return RandomPolicy(policy_mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 1\n",
    "\n",
    "tol = 1.0e-10\n",
    "gamma = 0.9\n",
    "\n",
    "num_steps: int = np.ceil(np.log(tol) / np.log(gamma)).astype(int)\n",
    "num_updates = 4_000\n",
    "\n",
    "trace_length = num_updates + num_steps\n",
    "k_lim = 200\n",
    "\n",
    "dlr = DynamicLearningRate(alpha = 0.03, beta = 0.5, H = 1_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_value_func = {\n",
    "    (state, action): 0.0\n",
    "    for state in si_mdp.non_terminal_states\n",
    "    for action in si_mdp.actions(state)\n",
    "}\n",
    "\n",
    "state_action_counter = {\n",
    "    (state, action): 0\n",
    "    for state in si_mdp.non_terminal_states\n",
    "    for action in si_mdp.actions(state)\n",
    "}\n",
    "\n",
    "\n",
    "while k < k_lim:\n",
    "\n",
    "    pol = assemble_eps_policy(k=k, Q_value_func=Q_value_func, mdp=si_mdp)\n",
    "    trace = si_mdp.simulate_actions(start_states=init_dist, policy=pol)\n",
    "\n",
    "    trace_vector = list(itertools.islice(trace, trace_length))\n",
    "    reward_vector = np.array([s.reward for s in trace_vector])\n",
    "    state_action_vector = [(s.state, s.action) for s in trace_vector]\n",
    "\n",
    "    gamma_arr = np.array([gamma ** k for k in np.arange(trace_length)])\n",
    "    discounted_reward = np.flip(\n",
    "        np.cumsum(np.flip(np.multiply(reward_vector, gamma_arr)))\n",
    "    )\n",
    "    G = np.multiply(discounted_reward, 1 / gamma_arr)\n",
    "\n",
    "    for idx, state_action in enumerate(state_action_vector[0:num_updates]):\n",
    "        state_action_counter[state_action] += 1\n",
    "        Q_value_func[tuple(state_action)] += (\n",
    "            dlr.evaluate(state_action_counter[state_action])\n",
    "        ) * (G[idx] - Q_value_func[state_action])\n",
    "\n",
    "    k += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(NonTerminal(state=InventoryState(on_hand=0, on_order=0)),\n",
       "  0): -41.76730144415136,\n",
       " (NonTerminal(state=InventoryState(on_hand=0, on_order=0)),\n",
       "  1): -34.864994559823174,\n",
       " (NonTerminal(state=InventoryState(on_hand=0, on_order=0)),\n",
       "  2): -35.80445346068443,\n",
       " (NonTerminal(state=InventoryState(on_hand=0, on_order=1)),\n",
       "  0): -31.97826805620582,\n",
       " (NonTerminal(state=InventoryState(on_hand=0, on_order=1)),\n",
       "  1): -27.427378392670715,\n",
       " (NonTerminal(state=InventoryState(on_hand=0, on_order=2)),\n",
       "  0): -28.463053851294546,\n",
       " (NonTerminal(state=InventoryState(on_hand=1, on_order=0)),\n",
       "  0): -32.94571005887549,\n",
       " (NonTerminal(state=InventoryState(on_hand=1, on_order=0)),\n",
       "  1): -28.635090707475776,\n",
       " (NonTerminal(state=InventoryState(on_hand=1, on_order=1)),\n",
       "  0): -28.84214548415869,\n",
       " (NonTerminal(state=InventoryState(on_hand=2, on_order=0)),\n",
       "  0): -29.69701922603516}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_value_func\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For State InventoryState(on_hand=0, on_order=0): Do Action 1\n",
      "For State InventoryState(on_hand=0, on_order=1): Do Action 1\n",
      "For State InventoryState(on_hand=0, on_order=2): Do Action 0\n",
      "For State InventoryState(on_hand=1, on_order=0): Do Action 1\n",
      "For State InventoryState(on_hand=1, on_order=1): Do Action 0\n",
      "For State InventoryState(on_hand=2, on_order=0): Do Action 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "opt_vf_pi, opt_policy_pi = policy_iteration_result(\n",
    "    si_mdp,\n",
    "    gamma=gamma\n",
    ")\n",
    "\n",
    "print(opt_policy_pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{NonTerminal(state=InventoryState(on_hand=0, on_order=0)): -34.894855781630035,\n",
       " NonTerminal(state=InventoryState(on_hand=0, on_order=1)): -27.660960231637507,\n",
       " NonTerminal(state=InventoryState(on_hand=0, on_order=2)): -27.991900091403533,\n",
       " NonTerminal(state=InventoryState(on_hand=1, on_order=0)): -28.660960231637507,\n",
       " NonTerminal(state=InventoryState(on_hand=1, on_order=1)): -28.991900091403533,\n",
       " NonTerminal(state=InventoryState(on_hand=2, on_order=0)): -29.991900091403533}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_vf_pi"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ed53eb0584307254b0c902844bcf8da812caec015d2a7bef039740197c0a4b91"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
