{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.dynamic_programming import V, S, A, extended_vf\n",
    "from rl import dynamic_programming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl import markov_process, markov_decision_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing import Mapping, Iterator, TypeVar, Tuple, Dict, Iterable\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from rl.distribution import Categorical, Choose, FiniteDistribution\n",
    "from rl.iterate import converged, iterate\n",
    "from rl.markov_process import NonTerminal, State\n",
    "from rl.markov_decision_process import (\n",
    "    FiniteMarkovDecisionProcess,\n",
    "    FiniteMarkovRewardProcess,\n",
    ")\n",
    "from rl.policy import FinitePolicy, FiniteDeterministicPolicy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.midterm_2022.priority_q import PriorityQueue\n",
    "from rl.midterm_2022 import grid_maze\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem we will implement the gaps-based value iteration algorithm mentioned in class.\n",
    "\n",
    "The gaps-based iteration algorithm proceeds as follows\n",
    "\n",
    "1. Initialize the value function to zero for all states: $v[s] = 0\\ \\forall s \\in \\mathcal{N}$\n",
    "2. Calculate the gaps for each state: $g[s] = |v[s] - \\max_a \\mathcal{R}(s,a) + \\sum_{s'} \\mathcal{P}(s,a,s') \\cdot v(s')|$\n",
    "3. While there is some gap that exceeds a threshold\n",
    " - Select the state with the  largest gap: $s_{max} = \\arg\\max_{s \\in \\mathcal{N}} g[s]$\n",
    " - Update the value function for $s_{max}$: $v[s_{max}] = \\max_a \\mathcal{R}(s_{max},a) + \\sum_{s'}\\mathcal{P}(s_{max},a,s') \\cdot v(s')$\n",
    " -  Update the gap for $s_{max}$: $g[s_{max}] = 0$\n",
    " - Oh yeah!\n",
    "4. Return v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will test your implementation on a grid maze MDP. We have defined this class in \"grid_maze.py\", you should  briefly familiarize yourself with that code. In particular pay attention to the difference in reward functions for the two classes \"GridMazeMDP_Dense\" and \"GridMazeMDP_Sparse\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how you can use the classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "underlying_maze = grid_maze.Maze(10, 10)\n",
    "maze_mdp = grid_maze.GridMazeMDP_Sparse(underlying_maze, 0, 0) # goal is at (0, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you can visualize the maze if you wish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "|*      | |         |\n",
      "| +-+-+ + +-+ +-+-+ +\n",
      "| | | |           | |\n",
      "| + + + +-+-+ +-+-+-+\n",
      "|     | |   |   |   |\n",
      "| +-+-+ + +-+ + +-+ +\n",
      "| |   |   | | | |   |\n",
      "| + + +-+-+ +-+-+ +-+\n",
      "|   | | | | |       |\n",
      "| +-+-+ + + +-+-+ + +\n",
      "|                 | |\n",
      "|-+ +-+-+-+ + +-+ +-+\n",
      "|         | | |     |\n",
      "| + +-+ + + +-+-+ +-+\n",
      "| |   | | |   |   | |\n",
      "| +-+-+ +-+-+ + +-+ +\n",
      "|   |   |     |     |\n",
      "|-+ +-+-+-+ + +-+-+ +\n",
      "|       |   |   |   |\n",
      "|-+-+-+-+-+-+-+-+-+-+\n"
     ]
    }
   ],
   "source": [
    "print(maze_mdp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you can also visualize a policy on the mdp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "|* < < <|v|> v < < <|\n",
      "| +-+-+ + +-+ +-+-+ +\n",
      "|^|v|v|^ < < < < <|^|\n",
      "| + + + +-+-+ +-+-+-+\n",
      "|^ < <|^|v <|^ <|> v|\n",
      "| +-+-+ + +-+ + +-+ +\n",
      "|^|v <|^ <|v|^|^|v <|\n",
      "| + + +-+-+ +-+-+ +-+\n",
      "|^ <|^|v|v|v|> > v <|\n",
      "| +-+-+ + + +-+-+ + +\n",
      "|^ < < < < < < < <|^|\n",
      "|-+ +-+-+-+ + +-+ +-+\n",
      "|> ^ < < <|^|^|> ^ <|\n",
      "| + +-+ + + +-+-+ +-+\n",
      "|^|^ <|^|^|^ <|> ^|v|\n",
      "| +-+-+ +-+-+ + +-+ +\n",
      "|^ <|> ^|> > ^|^ < <|\n",
      "|-+ +-+-+-+ + +-+-+ +\n",
      "|> ^ < <|> ^|^ <|> ^|\n",
      "|-+-+-+-+-+-+-+-+-+-+\n"
     ]
    }
   ],
   "source": [
    "v2_res = dynamic_programming.value_iteration_result(maze_mdp, 0.9)\n",
    "print(maze_mdp.print_policy(v2_res[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to make use of the PriorityQueue class in your implementation. A PriorityQueue is an ordered queue which supports the following operations\n",
    "1. isEmpty(self): check if the queue is empty   \n",
    "2. contains(self, element): check if the queue contains an element\n",
    "3. peek(self): peek at the highest priority element in the queue    \n",
    "4. pop(self): remove and return the highest priority element in the queue    \n",
    "5. insert(self, element, priority): insert an element into the queue with given priority\n",
    "6. update(self, element, new_priority): update the priority of an element in the queue\n",
    "7. delete(self, element): delete an element from the queue\n",
    "\n",
    "Below are some examples of using the queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True : the queue is empty\n",
      "False : the queue is not empty\n",
      "True : the queue contains a\n",
      "False : the queue does not contain a\n",
      "(1, 'a') : a is the first element in the queue\n",
      "True : the queue now contains b\n",
      "(0, 'b') : b is now at the front of the queue\n",
      "b : we removed b from the queue\n",
      "False : the queue still nonempty\n",
      "True : the queue still contains a\n",
      "False : the queue does not contain b anymore\n",
      "(1, 'a') : a is at the front of the queue\n",
      "(1, 'a') : a is still at the front of the queue\n",
      "(5, 'c') : after updating a is no longer at the front of the queue\n"
     ]
    }
   ],
   "source": [
    "q: PriorityQueue = PriorityQueue()\n",
    "print(q.isEmpty(), ':', \"the queue is empty\")\n",
    "q.insert(\"a\", 1)\n",
    "print(q.isEmpty(), ':',  \"the queue is not empty\")\n",
    "print(q.contains(\"a\"), ':',  \"the queue contains a\")\n",
    "print(q.contains(\"b\"), ':',  \"the queue does not contain a\")\n",
    "print(q.peek(), ':',  \"a is the first element in the queue\")\n",
    "q.insert(\"b\", 0)\n",
    "print(q.contains(\"b\"), ':',  \"the queue now contains b\")\n",
    "print(q.peek(), ':',  \"b is now at the front of the queue\")\n",
    "x = q.pop()\n",
    "print(x, ':',  \"we removed b from the queue\")\n",
    "print(q.isEmpty(), ':',  \"the queue still nonempty\")\n",
    "print(q.contains(\"a\"), ':',  \"the queue still contains a\")\n",
    "print(q.contains(\"b\"), ':',  \"the queue does not contain b anymore\")\n",
    "print(q.peek(), ':',  \"a is at the front of the queue\")\n",
    "q.insert(\"c\", 5)\n",
    "print(q.peek(), ':',  \"a is still at the front of the queue\")\n",
    "q.update(\"a\", 6)\n",
    "print(q.peek(), ':',  \"after updating a is no longer at the front of the queue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def invert_transition_map(\n",
    "    mdp: markov_decision_process.FiniteMarkovDecisionProcess[S, A]\n",
    ") -> Mapping[S, Iterable[S]]:\n",
    "    \"\"\"Map each state to its set of dependent states.\"\"\"\n",
    "    state_action_mapping = mdp.mapping\n",
    "    inverted_mapping = {state: set() for state in state_action_mapping.keys()}\n",
    "\n",
    "    for state in state_action_mapping.keys():\n",
    "        for action in state_action_mapping[state].keys():\n",
    "            dist = state_action_mapping[state][action]\n",
    "            \n",
    "            for reachable_state_reward_pair in dist.table().keys():\n",
    "                try:\n",
    "                    inverted_mapping[reachable_state_reward_pair[0]].add(state)\n",
    "                except KeyError:\n",
    "                    inverted_mapping[reachable_state_reward_pair[0]] = {state}\n",
    "\n",
    "    return inverted_mapping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_reward(\n",
    "    target_state: S,\n",
    "    mdp: FiniteMarkovDecisionProcess[S, A],\n",
    "    v: V[S],\n",
    "    gamma: float,\n",
    ") -> float:\n",
    "    \"\"\"Calculate the maximum value of Q(s, a) for a fixed s.\"\"\"\n",
    "    opt_reward = max(\n",
    "        mdp.mapping[target_state][a].expectation(\n",
    "            lambda s_r: s_r[1] + gamma * extended_vf(v, s_r[0])\n",
    "        )\n",
    "        for a in mdp.actions(target_state)\n",
    "    )\n",
    "    return opt_reward\n",
    "\n",
    "\n",
    "def gaps_value_iteration(\n",
    "    mdp: markov_decision_process.FiniteMarkovDecisionProcess[S, A],\n",
    "    gamma: float,\n",
    "    gaps: PriorityQueue,\n",
    ") -> Iterator[V[S]]:\n",
    "    \"\"\"\n",
    "    Calculate the value function (V*) of the given MDP by applying the\n",
    "    update function repeatedly until the values converge.\n",
    "\n",
    "    \"\"\"\n",
    "    dependency_map = invert_transition_map(mdp)\n",
    "    v_0: V[S] = {s: 0.0 for s in mdp.non_terminal_states}\n",
    "\n",
    "    def update(v: V[S]) -> V[S]:\n",
    "\n",
    "        target_state = NonTerminal(gaps.pop())\n",
    "        v[target_state] = optimal_reward(\n",
    "            target_state=target_state, mdp=mdp, v=v, gamma=gamma\n",
    "        )\n",
    "\n",
    "        for dependent_state in dependency_map[target_state]:\n",
    "            reward = np.abs(\n",
    "                v[dependent_state]\n",
    "                - optimal_reward(\n",
    "                    target_state=dependent_state,\n",
    "                    mdp=mdp,\n",
    "                    v=v,\n",
    "                    gamma=gamma,\n",
    "                )\n",
    "            )\n",
    "\n",
    "            if reward == 0.0 or reward == -0.0:\n",
    "                continue\n",
    "            else:\n",
    "\n",
    "                if gaps.contains(dependent_state.state):\n",
    "                    gaps.update(dependent_state.state, -reward)\n",
    "                else:\n",
    "                    gaps.insert(dependent_state.state, -reward)\n",
    "\n",
    "        return v\n",
    "\n",
    "    return iterate(update, v_0)\n",
    "\n",
    "\n",
    "def gaps_value_iteration_result(\n",
    "    mdp: FiniteMarkovDecisionProcess[S, A], gamma: float\n",
    ") -> Tuple[V[S], FiniteDeterministicPolicy[S, A]]:\n",
    "\n",
    "    gaps: PriorityQueue = PriorityQueue()\n",
    "\n",
    "    v: V[S] = {s: 0.0 for s in mdp.non_terminal_states}\n",
    "\n",
    "    # populate the queue\n",
    "    for state in v.keys():\n",
    "        state_gap = np.abs(optimal_reward(state, mdp, v, gamma) - v[state])\n",
    "        if state_gap != 0:  # no point adding states with zero gap!\n",
    "            gaps.insert(state.state, -state_gap)\n",
    "\n",
    "    # def criterion(x, y):\n",
    "    #     pass\n",
    "\n",
    "    # NOTE: Implementing a stopping condition using converged(), which is\n",
    "    # based on a boolean criterion function does not seem applicable here, because \n",
    "    # convergence in gap-value iteration should be defined by the priority queue, \n",
    "    # not the state of the value function.  Therefore, I implemented my own stopping \n",
    "    # condition:\n",
    "\n",
    "    for gv_iter in gaps_value_iteration(mdp, gamma, gaps):\n",
    "        if gaps.isEmpty():\n",
    "            break\n",
    "\n",
    "    opt_vf = gv_iter\n",
    "\n",
    "    opt_policy: markov_decision_process.FiniteDeterministicPolicy[\n",
    "        S, A\n",
    "    ] = dynamic_programming.greedy_policy_from_vf(mdp, opt_vf, gamma)\n",
    "\n",
    "    return opt_vf, opt_policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1_res = gaps_value_iteration_result(maze_mdp, 0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do not change the code below here, just run it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating the VF for a maze with sparse rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "underlying_maze = grid_maze.Maze(50, 50)\n",
    "maze_mdp = grid_maze.GridMazeMDP_Sparse(underlying_maze, 0, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### printing the runtime for the calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4800698757171631\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "v1_res = gaps_value_iteration_result(maze_mdp, 0.9)\n",
    "print(time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.735416889190674\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "v2_res = dynamic_programming.value_iteration_result(maze_mdp, 0.9)\n",
    "print(time() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### confirming that the value functions are identical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert v1_res[1] == v2_res[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating the VF for a maze with dense rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "maze_mdp = grid_maze.GridMazeMDP_Dense(underlying_maze, 0, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### printing the runtime for the calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.614510297775269\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "v1_res = gaps_value_iteration_result(maze_mdp, 1)\n",
    "print(time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.1651880741119385\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "v2_res = dynamic_programming.value_iteration_result(maze_mdp, 1)\n",
    "print(time() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### confirming that the value functions are identical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert v1_res[1] == v2_res[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
