{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Dict, Iterator, List, Mapping, Tuple\n",
    "from rl.distribution import Categorical\n",
    "from rl.dynamic_programming import value_iteration_result\n",
    "from rl.markov_decision_process import FiniteMarkovDecisionProcess\n",
    "import itertools as it\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem (3):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conventions used:\n",
    "\n",
    "We will represent a given roll of $N$ dice with a vector $\\vec{v}$ for which $v_i$ equals the number of rolls equal to $i+1$ (using zero-indexing).  Therefore, the vector $(1, 0, 3, 2)$ denotes a roll of one 1, three 3's, and two 4's.  This convention allows us to work with a reasonably-sized state space, and prevent an exponential explosion resulting from taking all possible rearrangements of dice into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roll_probability(roll: Tuple[int], K: int) -> float:\n",
    "    \"\"\"Calculate the probability of a given roll vector.\"\"\"\n",
    "    num_dice: int = sum(roll)\n",
    "    p: float = np.math.factorial(num_dice)\n",
    "\n",
    "    for roll_count in roll:\n",
    "        p /= np.math.factorial(roll_count)\n",
    "\n",
    "    p *= np.power(K, -float(num_dice))\n",
    "    return p\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Dicegame_State:\n",
    "    \"\"\"Class storing a state for our dice game.\"\"\"\n",
    "\n",
    "    hand: Tuple[int]\n",
    "    avail_rolls: Tuple[int]\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Roll_Choice:\n",
    "    \"\"\"Class storing dice choice as an action.\"\"\"\n",
    "\n",
    "    choice: Tuple[int]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Dicegame_StateAction_Transition:\n",
    "    \"\"\"Main class storing the state action map of our dice game.\"\"\"\n",
    "\n",
    "    N: int\n",
    "    K: int\n",
    "    C: int\n",
    "\n",
    "    @staticmethod\n",
    "    def build_counter_array(selected_idx: int, tup: Tuple[int]):\n",
    "        \"\"\"Create a vector storing the number of values for a given roll.\"\"\"\n",
    "        arr = np.array(tup)\n",
    "        arr[selected_idx] += 1\n",
    "        return tuple(arr)\n",
    "\n",
    "    def get_all_possible_choices(self, state: Dicegame_State) -> Iterator[Roll_Choice]:\n",
    "        \"\"\"Get all possible dice choices from a given state's available rolls.\"\"\"\n",
    "        return map(\n",
    "            Roll_Choice,\n",
    "            (\n",
    "                filter(\n",
    "                    lambda x: sum(x) > 0,\n",
    "                    it.product(*[np.arange(idx + 1) for idx in state.avail_rolls]),\n",
    "                )\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def get_stateaction_map(\n",
    "        self,\n",
    "    ) -> Mapping[Roll_Choice, Categorical[Tuple[Dicegame_State, float]]]:\n",
    "        \"\"\"Create the entire state action map.\"\"\"\n",
    "        self.stateaction_map = {}\n",
    "        all_rolls = set(\n",
    "            filter(\n",
    "                lambda x: sum(x) <= self.N,\n",
    "                it.product(*[np.arange(self.N + 1) for _ in range(self.K)]),\n",
    "            )\n",
    "        )\n",
    "        for hand in all_rolls:\n",
    "\n",
    "            for roll in filter(lambda x: sum(x) == (self.N - sum(hand)), all_rolls):\n",
    "\n",
    "                state = Dicegame_State(hand=hand, avail_rolls=roll)\n",
    "                for selection in self.get_all_possible_choices(state=state):\n",
    "                    self.add_stateaction(state=state, action=selection)\n",
    "                    assert state in self.stateaction_map.keys()\n",
    "\n",
    "        return self.stateaction_map\n",
    "\n",
    "    def add_stateaction(self, state: Dicegame_State, action: Roll_Choice) -> None:\n",
    "        \"\"\"Add a state-action to the state action map.\"\"\"\n",
    "        if state in self.stateaction_map.keys():\n",
    "\n",
    "            self.stateaction_map[state].update(\n",
    "                {action: self.prob(action=action, state=state)}\n",
    "            )\n",
    "        else:\n",
    "            self.stateaction_map[state] = {\n",
    "                action: self.prob(action=action, state=state)\n",
    "            }\n",
    "\n",
    "    def reward(self, next_state: Dicegame_State) -> int:\n",
    "        \"\"\"Calculate the reward at the end of the game.\"\"\"\n",
    "        if sum(next_state.hand) == self.N:\n",
    "            if next_state.hand[0] >= self.C:\n",
    "                return sum((i + 1) * next_state.hand[i] for i in np.arange(self.K))\n",
    "        return 0\n",
    "\n",
    "    def prob(\n",
    "        self, state: Dicegame_State, action: Roll_Choice\n",
    "    ) -> Categorical[Tuple[Dicegame_State, float]]:\n",
    "        \"\"\"Calculate the distribution of outcomes for a given state-action.\"\"\"\n",
    "        dist: Dict[Tuple[Dicegame_State, int], float] = {}\n",
    "        next_hand: Tuple[int] = tuple(np.array(state.hand) + np.array(action.choice))\n",
    "        num_remaining_dice: int = self.N - sum(state.hand) - sum(action.choice)\n",
    "\n",
    "        for next_roll in filter(\n",
    "            lambda x: sum(x) == num_remaining_dice,\n",
    "            it.product(*[np.arange(self.N) for _ in range(self.K)]),\n",
    "        ):\n",
    "            next_state = Dicegame_State(hand=next_hand, avail_rolls=next_roll)\n",
    "\n",
    "            p: float = np.math.factorial(num_remaining_dice)\n",
    "\n",
    "            for roll_count in next_roll:\n",
    "                p /= np.math.factorial(roll_count)\n",
    "\n",
    "            p *= np.power(self.K, -float(num_remaining_dice))\n",
    "\n",
    "            dist[next_state, self.reward(next_state=next_state)] = p\n",
    "\n",
    "        return Categorical(dist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.2.1 : Expected reward when playing optimally : 18.390390253776786\n",
      "3.2.2 : Optimal action when the first roll is (1, 2, 2, 3, 3, 4) : (0, 0, 0, 1)\n"
     ]
    }
   ],
   "source": [
    "d = Dicegame_StateAction_Transition(N=6, K=4, C=1)\n",
    "mdp = FiniteMarkovDecisionProcess(d.get_stateaction_map)\n",
    "\n",
    "opt_v, opt_policy = value_iteration_result(mdp=mdp, gamma=1.0)\n",
    "\n",
    "# calculate the expected reward when starting from scratch: state.hand == (0, 0, 0, 0)\n",
    "expected_reward: float = 0.0\n",
    "for init_state in list(filter(lambda x: x.state.hand == (0, 0, 0, 0), opt_v.keys())):\n",
    "    expected_reward += opt_v[init_state] * roll_probability(\n",
    "        init_state.state.avail_rolls, K=4\n",
    "    )\n",
    "\n",
    "print(f\"3.2.1 : Expected reward when playing optimally : {expected_reward}\")\n",
    "\n",
    "# a roll of (1, 2, 2, 3, 3, 4) is represented as (1, 2, 2, 1)\n",
    "target_state = list(\n",
    "    filter(\n",
    "        lambda x: x.hand == (0, 0, 0, 0) and x.avail_rolls == (1, 2, 2, 1),\n",
    "        opt_policy.action_for.keys(),\n",
    "    )\n",
    ")[0]\n",
    "opt_action = opt_policy.action_for[target_state]\n",
    "\n",
    "print(\n",
    "    f\"3.2.2 : Optimal action when the first roll is (1, 2, 2, 3, 3, 4) : {opt_action.choice}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, for question 3.2.2., if we encounter the roll (1, 2, 2, 3, 3, 4) as our first roll, the optimal action is to choose the single die that rolled 4."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ed53eb0584307254b0c902844bcf8da812caec015d2a7bef039740197c0a4b91"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
