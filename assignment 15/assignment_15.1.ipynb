{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import choices\n",
    "from typing import Sequence, Tuple, Mapping\n",
    "\n",
    "S = str\n",
    "DataType = Sequence[Sequence[Tuple[S, float]]]\n",
    "ProbFunc = Mapping[S, Mapping[S, float]]\n",
    "RewardFunc = Mapping[S, float]\n",
    "ValueFunc = Mapping[S, float]\n",
    "\n",
    "\n",
    "def get_state_return_samples(data: DataType) -> Sequence[Tuple[S, float]]:\n",
    "    \"\"\"\n",
    "    prepare sequence of (state, return) pairs.\n",
    "    Note: (state, return) pairs is not same as (state, reward) pairs.\n",
    "    \"\"\"\n",
    "    return [(s, sum(r for (_, r) in l[i:])) for l in data for i, (s, _) in enumerate(l)]\n",
    "\n",
    "\n",
    "def get_mc_value_function(state_return_samples: Sequence[Tuple[S, float]]) -> ValueFunc:\n",
    "    \"\"\"\n",
    "    Implement tabular MC Value Function compatible with the interface defined above.\n",
    "    \"\"\"\n",
    "    sr_dict = {}\n",
    "\n",
    "    for (state, ret) in state_return_samples:\n",
    "        if state not in sr_dict.keys():\n",
    "            sr_dict[state] = [ret]\n",
    "        else:\n",
    "            sr_dict[state].append(ret)\n",
    "    return {state: np.mean(sr_dict[state]) for state in sr_dict.keys()}\n",
    "\n",
    "\n",
    "def get_state_reward_next_state_samples(data: DataType) -> Sequence[Tuple[S, float, S]]:\n",
    "    \"\"\"\n",
    "    prepare sequence of (state, reward, next_state) triples.\n",
    "    \"\"\"\n",
    "    return [\n",
    "        (s, r, l[i + 1][0] if i < len(l) - 1 else \"T\")\n",
    "        for l in data\n",
    "        for i, (s, r) in enumerate(l)\n",
    "    ]\n",
    "\n",
    "\n",
    "def get_probability_and_reward_functions(\n",
    "    srs_samples: Sequence[Tuple[S, float, S]]\n",
    ") -> Tuple[ProbFunc, RewardFunc]:\n",
    "    \"\"\"\n",
    "    Implement code that produces the probability transitions and the\n",
    "    reward function compatible with the interface defined above.\n",
    "    \"\"\"\n",
    "    p_dict = {}\n",
    "    r_dict = {}\n",
    "    for (init_state, reward, next_state) in srs_samples:\n",
    "        if init_state not in p_dict.keys():\n",
    "            p_dict[init_state] = {next_state: 1}\n",
    "        elif next_state not in p_dict[init_state].keys():\n",
    "            p_dict[init_state][next_state] = 1\n",
    "        else:\n",
    "            p_dict[init_state][next_state] += 1\n",
    "\n",
    "        if init_state not in r_dict.keys():\n",
    "            r_dict[init_state] = [reward]\n",
    "        else:\n",
    "            r_dict[init_state].append(reward)\n",
    "\n",
    "    for init_state in p_dict.keys():\n",
    "        r_dict[init_state] = np.mean(r_dict[init_state])\n",
    "\n",
    "    return (p_dict, r_dict)\n",
    "\n",
    "\n",
    "def get_mrp_value_function(prob_func: ProbFunc, reward_func: RewardFunc) -> ValueFunc:\n",
    "    \"\"\"\n",
    "    Implement code that calculates the MRP Value Function from the probability\n",
    "    transitions and reward function, compatible with the interface defined above.\n",
    "    Hint: Use the MRP Bellman Equation and simple linear algebra\n",
    "    \"\"\"\n",
    "    nonterminal_states = [key for key in prob_func.keys()]\n",
    "    tots = [\n",
    "        np.sum(\n",
    "            [prob_func[init_state][next_state] for next_state in prob_func[init_state]]\n",
    "        )\n",
    "        for init_state in nonterminal_states\n",
    "    ]\n",
    "    nnt = len(nonterminal_states)\n",
    "\n",
    "    P_mat = np.zeros((nnt, nnt))\n",
    "    R_vec = np.zeros(nnt)\n",
    "    row_idx = 0\n",
    "\n",
    "    for init_state in nonterminal_states:\n",
    "        R_vec[row_idx] = reward_func[init_state]\n",
    "        col_idx = 0\n",
    "        for next_state in nonterminal_states:\n",
    "            P_mat[row_idx][col_idx] = prob_func[init_state][next_state]\n",
    "            col_idx += 1\n",
    "        row_idx += 1\n",
    "    P_mat = P_mat.T\n",
    "    P_mat /= tots\n",
    "    lhs = np.eye(nnt) - P_mat.T\n",
    "    v_vec = np.linalg.solve(lhs, R_vec)\n",
    "    idx = 0\n",
    "    vf = {}\n",
    "    for state in nonterminal_states:\n",
    "        vf[state] = v_vec[idx]\n",
    "        idx += 1\n",
    "    return vf\n",
    "\n",
    "\n",
    "def get_td_value_function(\n",
    "    srs_samples: Sequence[Tuple[S, float, S]],\n",
    "    num_updates: int = 300000,\n",
    "    learning_rate: float = 0.03,\n",
    "    learning_rate_decay: int = 30,\n",
    ") -> ValueFunc:\n",
    "    \"\"\"\n",
    "    Implement tabular TD(0) (with experience replay) Value Function compatible\n",
    "    with the interface defined above. Let the step size (alpha) be:\n",
    "    learning_rate * (updates / learning_rate_decay + 1) ** -0.5\n",
    "    so that Robbins-Monro condition is satisfied for the sequence of step sizes.\n",
    "    \"\"\"\n",
    "    vf = {}\n",
    "    for srs in srs_samples:\n",
    "        vf[srs[0]] = 0.0\n",
    "        vf[srs[2]] = 0.0\n",
    "\n",
    "    update_num = 1\n",
    "\n",
    "    def evaluate_step_size(update_num):\n",
    "        return learning_rate * ((update_num - 1) / learning_rate_decay + 1) ** -0.5\n",
    "\n",
    "    for (init_state, reward, next_state) in choices(srs_samples, k=num_updates):\n",
    "        vf[init_state] += evaluate_step_size(update_num) * (\n",
    "            reward + vf[next_state] - vf[init_state]\n",
    "        )\n",
    "        update_num += 1\n",
    "\n",
    "    return vf\n",
    "\n",
    "\n",
    "def get_lstd_value_function(srs_samples: Sequence[Tuple[S, float, S]]) -> ValueFunc:\n",
    "    \"\"\"\n",
    "    Implement LSTD Value Function compatible with the interface defined above.\n",
    "    Hint: Tabular is a special case of linear function approx where each feature\n",
    "    is an indicator variables for a corresponding state and each parameter is\n",
    "    the value function for the corresponding state.\n",
    "    \"\"\"\n",
    "    \n",
    "    vf = {}\n",
    "    for srs in srs_samples:\n",
    "        vf[srs[0]] = 0.0\n",
    "        vf[srs[2]] = 0.0\n",
    "\n",
    "    nnt = len(vf.keys())\n",
    "\n",
    "    A = 0.001 * np.eye(nnt)\n",
    "    K = 100_000\n",
    "\n",
    "    phi = {}\n",
    "    for idx, state in enumerate(vf.keys()):\n",
    "        phi[state] = np.eye(nnt)[:,idx]\n",
    "\n",
    "    b = np.zeros(nnt)\n",
    "    for srs in choices(srs_samples, k=K):\n",
    "        init_state = srs[0]\n",
    "        r = srs[1]\n",
    "        next_state = srs[2]\n",
    "        A += np.outer(phi[init_state], (phi[init_state] - phi[next_state]).T)\n",
    "        b += phi[init_state] * r\n",
    "    A /= K\n",
    "    b /= K\n",
    "    w = np.linalg.solve(A, b)\n",
    "    for idx, state in enumerate(vf.keys()):\n",
    "        vf[state] = w[idx]\n",
    "    return vf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "given_data: DataType = [\n",
    "    [('A', 2.), ('A', 6.), ('B', 1.), ('B', 2.)],\n",
    "    [('A', 3.), ('B', 2.), ('A', 4.), ('B', 2.), ('B', 0.)],\n",
    "    [('B', 3.), ('B', 6.), ('A', 1.), ('B', 1.)],\n",
    "    [('A', 0.), ('B', 2.), ('A', 4.), ('B', 4.), ('B', 2.), ('B', 3.)],\n",
    "    [('B', 8.), ('B', 2.)]\n",
    "]\n",
    "\n",
    "sr_samps = get_state_return_samples(given_data)\n",
    "\n",
    "\n",
    "\n",
    "srs_samps = get_state_reward_next_state_samples(given_data)\n",
    "\n",
    "pfunc, rfunc = get_probability_and_reward_functions(srs_samps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------- MONTE CARLO VALUE FUNCTION --------------\n",
      "{'A': 9.571428571428571, 'B': 5.642857142857143}\n",
      "-------------- MRP VALUE FUNCTION ----------\n",
      "{'A': 12.933333333333332, 'B': 9.6}\n",
      "------------- TD VALUE FUNCTION --------------\n",
      "{'A': 12.848679979954118, 'B': 9.610724118089085, 'T': 0.0}\n",
      "------------- LSTD VALUE FUNCTION --------------\n",
      "{'A': 12.894341269082576, 'B': 9.566396824829738, 'T': 0.0}\n"
     ]
    }
   ],
   "source": [
    "print(\"------------- MONTE CARLO VALUE FUNCTION --------------\")\n",
    "print(get_mc_value_function(sr_samps))\n",
    "\n",
    "print(\"-------------- MRP VALUE FUNCTION ----------\")\n",
    "print(get_mrp_value_function(pfunc, rfunc))\n",
    "\n",
    "print(\"------------- TD VALUE FUNCTION --------------\")\n",
    "print(get_td_value_function(srs_samps))\n",
    "\n",
    "print(\"------------- LSTD VALUE FUNCTION --------------\")\n",
    "print(get_lstd_value_function(srs_samps))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ed53eb0584307254b0c902844bcf8da812caec015d2a7bef039740197c0a4b91"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
